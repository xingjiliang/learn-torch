{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb35e6ef-1896-41c5-95e3-cc05243dfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5adfb-5c4c-492e-9d0c-f4f0df738ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    #存在的问题是没搞懂sum时的dim应该指定为什么，比如X形状为[2,3,4]，那么sum指定dim为-1，形状会变成[2,3,1]\n",
    "    def forward(self, X):\n",
    "        X_exp = X.exp()\n",
    "        X_exp_sum = X_exp.sum(self.dim, keepdim=True)\n",
    "        return X_exp/X_exp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba49fad-cb08-4e90-9858-5b5e0819bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_features, out_features, head=8):\n",
    "        super().__init__()\n",
    "        #self.head = head\n",
    "        # nn.Parameter的requires_grad默认为True\n",
    "        self.multiWq = nn.Parameter(torch.randn(head, in_features, out_features))\n",
    "        self.multiWk = nn.Parameter(torch.randn(head, in_features, out_features))\n",
    "        self.multiWv = nn.Parameter(torch.randn(head, in_features, out_features))\n",
    "        self.sqrt_d = torch.sqrt(out_features)\n",
    "        #for i in range(head):\n",
    "        #    self.Wqs.append(nn.Linear(in_features=in_features, out_features=out_features))\n",
    "        \n",
    "    def forward(X):\n",
    "        # 此处省略bias\n",
    "        # X.shape=[batch_size(samples), len, 1(head), 1, in_features], multiWq.shape=[head, in_features, out_features],目标是生成[samples, len, head, out_features]?\n",
    "        X_unsqueeze = X.unsqueeze(-2).unsqueeze(-2)\n",
    "        Q = X_unsqueeze.matmul(self.multiWq).squeeze()\n",
    "        K = X_unsqueeze.matmul(self.multiWk).squeeze()\n",
    "        V = X_unsqueeze.matmul(self.multiWv).squeeze()\n",
    "\n",
    "    def get_attention_weight(Q, K):\n",
    "        softmax = Softmax(-1)\n",
    "        return softmax(Q.unsqueeze(-2).matmul(K.unsqueeze(-1)).squeeze())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084ade9-770c-4825-ac9d-7a0a9bc38917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(X):\n",
    "        pass\n",
    "        \n",
    "class AddRes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(X):\n",
    "        pass\n",
    "\n",
    "class PWFCN():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(X):\n",
    "        pass\n",
    "\n",
    "batch_size = 2\n",
    "len = 10\n",
    "word_dim = 32\n",
    "X = torch.randn(batch_size, len, word_dim)\n",
    "my_transformer_encoder = nn.Sequential(SelfAttention(), AddRes(), LayerNorm(), PWFCN(), AddRes(), LayerNorm())\n",
    "output = my_transformer_encoder(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
